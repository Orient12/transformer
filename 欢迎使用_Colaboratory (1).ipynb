{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "欢迎使用 Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>欢迎使用 Colaboratory！</h1>\n",
        "\n",
        "\n",
        "Colaboratory 是一个免费的 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。\n",
        "\n",
        "借助 Colaboratory，您可以编写和执行代码、保存和共享分析结果，以及利用强大的计算资源，所有这些都可通过浏览器免费使用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzU2_AH-gIA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "fdf5a69d-13e6-4b77-c20b-8d24383b9250"
      },
      "source": [
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install tqdm\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install nltk==3.4.1\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.4)\n",
            "Requirement already satisfied: torchvision==0.3.0 from https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.16.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Collecting nltk==3.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/56/90178929712ce427ebad179f8dc46c8deef4e89d4c853092bee1efd57d05/nltk-3.4.1.zip (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.1) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/8a/10/d646015f33c525688e91986c4544c68019b19a473cb33d3b55\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e0BkQtFfxtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "2de520f7-3bdb-49ae-8d13-a2f571d0c99b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xt4_TOIgkWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import json\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz6UoTZ9gzEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class constants():\n",
        "  def __init__(self):\n",
        "    self.PAD = 0\n",
        "    self.UNK = 1\n",
        "    self.BOS = 2\n",
        "    self.EOS = 3\n",
        "\n",
        "    self.PAD_WORD = '<blank>'\n",
        "    self.UNK_WORD = '<unk>'\n",
        "    self.BOS_WORD = '<s>'\n",
        "    self.EOS_WORD = '</s>'\n",
        "Constants = constants()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anwQxPZ_hWv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(torch.nn.Module):\n",
        "    def __init__(self, model_dim=512, headed_count=8, dropout=0.0):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.headed_count = headed_count\n",
        "        self.head_dim = model_dim//headed_count\n",
        "        self.model_dim = model_dim\n",
        "        self.linear_q = torch.nn.Linear(model_dim, headed_count * self.head_dim)\n",
        "        self.linear_k = torch.nn.Linear(model_dim, headed_count * self.head_dim)\n",
        "        self.linear_v = torch.nn.Linear(model_dim, headed_count * self.head_dim)\n",
        "        self.linear_final = torch.nn.Linear(model_dim, model_dim)\n",
        "        self.ScaledDotProductAttention = ScaledDotProductAttention(dropout)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.layer_norm = torch.nn.LayerNorm(model_dim)\n",
        "    def forward(self, query, key, value, Mask=None):\n",
        "\n",
        "        \"\"\"\n",
        "        :param X: 输入张量， （Batch_size, Sentence_Length, model_dim）\n",
        "        :param Mask: Mask张量， (Sentence_Length, Sentence_Length)\n",
        "        :return: output, (batch_size, Sentence_Length, model_dim)\n",
        "        \"\"\"\n",
        "        X_size = query.size()\n",
        "        batch_size = X_size[0]\n",
        "        sentence_length = X_size[1]\n",
        "        model_dim = X_size[2]\n",
        "\n",
        "        residual = query\n",
        "\n",
        "        #生成query,key,value向量\n",
        "        query = self.linear_q(query)\n",
        "        key = self.linear_k(key)\n",
        "        value = self.linear_v(value)\n",
        "\n",
        "        #分出headed_count个头\n",
        "        query = query.view(batch_size * self.headed_count, -1, self.head_dim)\n",
        "        key = key.view(batch_size * self.headed_count, -1, self.head_dim)\n",
        "        value = value.view(batch_size * self.headed_count, -1, self.head_dim)\n",
        "\n",
        "        # if Mask:\n",
        "        Mask = Mask.repeat(self.headed_count, 1, 1)\n",
        "\n",
        "        scale = self.head_dim ** -0.5\n",
        "\n",
        "        #ScaledDotAttention\n",
        "        context, attention = self.ScaledDotProductAttention(query, key, value, scale=scale, Mask=Mask)\n",
        "\n",
        "        #concat head\n",
        "        context = context.view(batch_size, -1, self.headed_count * self.head_dim)\n",
        "\n",
        "        output = self.linear_final(context)\n",
        "\n",
        "        output = self.layer_norm(residual + output)\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nQ_aXD2hcgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotProductAttention(torch.nn.Module):\n",
        "    def __init__(self, attention_dropout = 0.0):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(attention_dropout)\n",
        "        self.softmax = torch.nn.Softmax(dim = 2)\n",
        "    def forward(self, q, k, v, scale=None, Mask=None):\n",
        "\n",
        "        \"\"\"\n",
        "        :param q:查询向量，（Batch_size, Sentence_Length, model_dim/headed_count）\n",
        "        :param k: 键向量，（Batch_size, Sentence_Length, model_dim/headed_count）\n",
        "        :param v: 值向量，（Batch_size, Sentence_Length, model_dim/headed_count）\n",
        "        :param scale: 缩放因子，浮点标量\n",
        "        :param Mask: Mask矩阵，对Attention矩阵进行Mask,与Attention矩阵的维度相同\n",
        "        :return: Context，上下文张量；Attention,Attention张量\n",
        "        \"\"\"\n",
        "        Attention = torch.bmm(q, k.transpose(1, 2))\n",
        "        if scale:\n",
        "            Attention *= scale\n",
        "        # if Mask:\n",
        "        #print(Attention.size(), Mask.size())\n",
        "        Attention = Attention.masked_fill(Mask, -np.inf)\n",
        "\n",
        "        Attention = self.softmax(Attention)\n",
        "        Attention = self.dropout(Attention)\n",
        "        context = torch.bmm(Attention, v)\n",
        "\n",
        "        return context, Attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boCnD8rMhexc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class embedding_layer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, model_dim):\n",
        "        super(embedding_layer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.model_dim = model_dim\n",
        "        self.Embedding_Matrix = torch.nn.Embedding(vocab_size, model_dim)\n",
        "    def forward(self, X):\n",
        "        return self.Embedding_Matrix(X) * (self.model_dim ** (0.5))\n",
        "    def share_weight_linear(self, X):\n",
        "        \"\"\"\n",
        "        :param X:Decoder输出，（batch_size, sentence_length, model_dim）\n",
        "        :return: (batch_size, sentence_length, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size = X.size(0)\n",
        "        sentence_length = X.size(1)\n",
        "        model_dim = X.size(2)\n",
        "        out = X.view(-1, model_dim)\n",
        "        out = torch.mm(out, self.Embedding_Matrix.weight.permute(1, 0))\n",
        "        out = out.view(batch_size, sentence_length, self.vocab_size)\n",
        "        return  out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB7IVXqUhiT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionEmbedding(torch.nn.Module):\n",
        "    def __init__(self, Max_Sentence_Length, Model_dim):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "\n",
        "        #生成位置嵌入矩阵\n",
        "        position_encoding = np.array([[pos / np.power(10000, 2.0 * (i // 2)/Model_dim) for i in range(Model_dim)]for pos in range(Max_Sentence_Length)])\n",
        "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
        "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
        "        position_encoding = torch.Tensor(position_encoding)\n",
        "\n",
        "        pad_row =  torch.zeros((1, Model_dim))\n",
        "\n",
        "        position_encoding = torch.cat((pad_row, position_encoding), dim=0)\n",
        "\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(position_encoding, freeze=True)\n",
        "\n",
        "    def forward(self, input_len):\n",
        "        \"\"\"\n",
        "        :param input:一个batch的句子长度张量，（batch_size, 1）\n",
        "        :return: 一个batch句子的位置嵌入矩阵，（batch_size, max(input_len), Model_dim）\n",
        "        \"\"\"\n",
        "\n",
        "        max_len = torch.max(input_len)\n",
        "        position = torch.LongTensor([list(range(1, k+1)) + [0] * (max_len - k).item() for k in input_len]).cuda()\n",
        "        out = self.embedding(position)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-hL7t6Zhj95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding_mask(seq_k, seq_q):\n",
        "    len_q = seq_q.size(1)\n",
        "    pad_mask = seq_k.eq(0)\n",
        "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)  # shape [B, L_q, L_k]\n",
        "    return pad_mask\n",
        "def sequence_mask(seq):\n",
        "    batch_size, seq_len = seq.size()\n",
        "    mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),\n",
        "                    diagonal=1)\n",
        "    mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, L]\n",
        "    return mask\n",
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, model_dim=512, headed_count=8, ffn_dim=2048, dropout=0.0):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.MultiHeadedAttention = MultiHeadedAttention(model_dim, headed_count, dropout)\n",
        "        self.FeedForward = FeedForward(model_dim, ffn_dim)\n",
        "    def forward(self, X, Mask=None):\n",
        "        out, attention = self.MultiHeadedAttention(X, X, X, Mask)\n",
        "        out = self.FeedForward(out)\n",
        "        return out, attention\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, Max_Sentence_Length, num_layers=6, model_dim=512, headed_count=8, ffn_dim=2048, dropout=0.0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder = torch.nn.ModuleList(\n",
        "            [EncoderLayer(model_dim, headed_count, ffn_dim, dropout)  for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.position_embedding = PositionEmbedding(Max_Sentence_Length=Max_Sentence_Length, Model_dim=model_dim)\n",
        "        #self.word_embedding = torch.nn.Embedding(source_vocab_size, model_dim)#Word_Embedding(pretrained)\n",
        "\n",
        "    def forward(self, X, padding_mask):\n",
        "\n",
        "        #word_embedding = self.word_embedding(X)\n",
        "        #position_embedding = self.position_embedding(input_len)\n",
        "        #out = word_embedding + position_embedding\n",
        "\n",
        "        attentions = []\n",
        "        for encoder in self.encoder:\n",
        "            out, attention = encoder(X, padding_mask)\n",
        "            attentions.append(attention)\n",
        "\n",
        "        return out, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6feupnghmrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, model_dim, head_count=8, ffn_dim=2048, dropout=0.0):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.EncoderDecoderAttention = MultiHeadedAttention(model_dim, head_count, dropout)\n",
        "        self.Self_Attention = MultiHeadedAttention(model_dim, head_count, dropout)\n",
        "        self.FeedForward = FeedForward(model_dim, ffn_dim, dropout)\n",
        "    def forward(self, encoder_output, X=None, self_attention_mask=None, encoder_decoder_attention_mask=None):\n",
        "        \"\"\"\n",
        "        :param X:上一层decoder的输出或者词向量； （batch_size, sentence_length, model_dim）\n",
        "        :param encoder_output: encoder的输出； (batch_size, sentence_length, model_dim)\n",
        "        :param mask_pad: 句子补齐产生的0向量， 负无穷填充； (batch_size, sentence_length, sentence_length)\n",
        "        :param mask_sequence: attention执行过程为防止前面的词语可以看到后面的词语进行负无穷填充； (batch_size, sentence_length, sentence_length)\n",
        "        :return: 一层decoder的输出，self-attention矩阵，EncoderDecoder_Attention矩阵\n",
        "        \"\"\"\n",
        "        decoder_output, Self_attention = self.Self_Attention(X, X, X, self_attention_mask)\n",
        "\n",
        "        decoder_output, EncoderDecoder_attention = self.EncoderDecoderAttention(decoder_output, encoder_output, encoder_output, encoder_decoder_attention_mask)\n",
        "\n",
        "        output = self.FeedForward(decoder_output)\n",
        "\n",
        "        return output, Self_attention, EncoderDecoder_attention\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, num_layer=6, model_dim=512, head_count=8, ffn_dim=2048, dropout=0.0):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.num_layer = num_layer\n",
        "        # self.word_embedding = WordEmbedding(pretrained)\n",
        "        # self.Position_Embedding = PositionEmbedding(max_sentence_length, model_dim)\n",
        "        self.decoder_layer = torch.nn.ModuleList([DecoderLayer(model_dim, head_count, ffn_dim, dropout) for _ in range(num_layer)])\n",
        "\n",
        "    def forward(self, encoder_output, self_attention_mask, encoder_decoder_attention_mask, X=None):\n",
        "        \"\"\"\n",
        "        :param X:目标语言输入； （batch_size, Sentence_Length）\n",
        "        :param input_len: 目标语言句子长度，一个batch中每个句子的长度；（batch_size, 1）\n",
        "        :param encoder_output: encoder的输出；（batch_size, model_dim）\n",
        "        :return: Decoder输出\n",
        "        注：这里的padding有点特殊，在self-attention应用padding_mask和sequence_mask的加和，在encoder-decoder attention中只应用padding_mask\n",
        "        \"\"\"\n",
        "        output = X\n",
        "        #mask = torch.gt((mask_padding+mask_context), 0)\n",
        "\n",
        "        self_attention = []\n",
        "        encoderdecoder_attention = []\n",
        "        for decoder in self.decoder_layer:\n",
        "            output, Self_attention, EncoderDecoder_attention = decoder(encoder_output, output, self_attention_mask, encoder_decoder_attention_mask)\n",
        "            self_attention.append(Self_attention)\n",
        "            encoderdecoder_attention.append(EncoderDecoder_attention)\n",
        "\n",
        "        return output, self_attention, encoderdecoder_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhl-OmHWhpsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.w1 = torch.nn.Conv1d(model_dim, ffn_dim, 1)\n",
        "        self.w2 = torch.nn.Conv1d(ffn_dim, model_dim, 1)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.layer_norm = torch.nn.LayerNorm(model_dim)\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        :param X:输入， （Batch_size, Sentence_Length, model_dim）\n",
        "        :return: 输出， （Batch_size, Sentence_Length, model_dim）\n",
        "\n",
        "        \"\"\"\n",
        "        output = self.w2(F.relu(self.w1(X.transpose(1, 2))))\n",
        "        output = output.transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        output = self.layer_norm(X+output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA9SYpZthriA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients by the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w7Yba9thtbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding_mask(seq_k, seq_q):\n",
        "    len_q = seq_q.size(1)\n",
        "    pad_mask = seq_k.eq(0)\n",
        "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)  # shape [B, L_q, L_k]\n",
        "    return pad_mask.cuda()\n",
        "def sequence_mask(seq):\n",
        "    batch_size, seq_len = seq.size()\n",
        "    mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.uint8),\n",
        "                    diagonal=1)\n",
        "    mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, L]\n",
        "    return mask.cuda()\n",
        "class transformer(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, max_sentence_length, num_layers=6, model_dim=512, headed_count=8, ffn_dim=2048, dropout=0.0):\n",
        "        super(transformer, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.Encoder = Encoder(max_sentence_length, num_layers, model_dim, headed_count, ffn_dim, dropout)\n",
        "        self.Decoder = Decoder(num_layers, model_dim, headed_count, ffn_dim, dropout)\n",
        "        self.Position_Embedding = PositionEmbedding(max_sentence_length, model_dim)\n",
        "        self.embedding_layer = embedding_layer(vocab_size, model_dim)\n",
        "        #self.linear = torch.nn.Linear(model_dim, vocab_size)\n",
        "    def forward(self, source_word, source_len, target_word, target_len):\n",
        "        #生成mask\n",
        "\n",
        "        source_self_attention_mask = padding_mask(source_word, source_word)\n",
        "\n",
        "        target_padding_mask = padding_mask(target_word, target_word)\n",
        "        target_sequence_mask = sequence_mask(target_word)\n",
        "        target_self_attention_mask = (target_padding_mask + target_sequence_mask).gt(0)\n",
        "        target_encoder_decoder_mask = padding_mask(source_word, target_word)\n",
        "\n",
        "        #生成嵌入\n",
        "        source_word_embedding = self.embedding_layer(source_word)\n",
        "        source_position_embedding = self.Position_Embedding(source_len)\n",
        "        source_embedding = source_word_embedding + source_position_embedding\n",
        "        target_word_embedding = self.embedding_layer(target_word)\n",
        "        target_position_embedding = self.Position_Embedding(target_len)\n",
        "        target_embedding = target_word_embedding + target_position_embedding\n",
        "\n",
        "        #encoder\n",
        "        encoder_output, encoder_attention = self.Encoder(source_embedding, source_self_attention_mask)\n",
        "\n",
        "        #decoder\n",
        "        decoder_output, decoder_self_attention, decoder_encoder_decoder_attention = self.Decoder(encoder_output, target_self_attention_mask, target_encoder_decoder_mask, target_embedding)\n",
        "\n",
        "        #生成logits\n",
        "\n",
        "        logits = self.embedding_layer.share_weight_linear(decoder_output) * (self.model_dim ** -0.5)\n",
        "        #logits = self.linear(decoder_output)\n",
        "        # del source_self_attention_mask, target_self_attention_mask, target_encoder_decoder_mask\n",
        "        # del target_sequence_mask, target_padding_mask, source_embedding\n",
        "        # del source_position_embedding, target_embedding, target_position_embedding, encoder_output\n",
        "\n",
        "        return logits, encoder_attention, decoder_self_attention, decoder_encoder_decoder_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVHjhTXehy-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(logits, target, smoothing, vocabsize):\n",
        "    t1 = target.size(0)\n",
        "    t2 = target.size(1)\n",
        "    confidence = 1 - smoothing\n",
        "    low_confidence = (1 - confidence)/(vocabsize - 1)\n",
        "    gold = torch.zeros_like(logits).scatter(2, target.unsqueeze(-1), 1)\n",
        "    gold = confidence * gold + low_confidence * (1 - gold)\n",
        "    logits_softmax = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    non_pad_mask = target.ne(Constants.PAD)\n",
        "    count = non_pad_mask.sum(dim=-1).sum(dim=-1)\n",
        "    loss = -(logits_softmax * gold).sum(dim=-1)\n",
        "    loss = loss.masked_select(non_pad_mask).sum()\n",
        "\n",
        "    return loss/count\n",
        "def get_bleu(logits, target, index2word):\n",
        "    pre = torch.argmax(logits, dim=2)\n",
        "    all_bleu = []\n",
        "    for i in range(logits.size(0)):\n",
        "        candidate = [index2word[str(k.item())] for k in pre[i]]\n",
        "        reference = [index2word[str(k.item())] for k in target[i] if k.item()!= Constants.PAD]\n",
        "        if Constants.EOS_WORD in candidate:c_end = candidate.index(Constants.EOS_WORD)\n",
        "        else:continue\n",
        "        can = candidate[:c_end+1]\n",
        "\n",
        "        if Constants.EOS_WORD in reference:r_end = reference.index(Constants.EOS_WORD)\n",
        "        else:continue\n",
        "        ref = reference[:r_end+1]\n",
        "#         print(can, ref)\n",
        "#         print(pre)\n",
        "        bleu = sentence_bleu([ref], can)\n",
        "        all_bleu.append(bleu)\n",
        "    return np.average(all_bleu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h96jQbGMh8nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_from_file(path, max_sentence_length, keep_case):\n",
        "    All = []\n",
        "    trimed_sentence_count = 0\n",
        "    with open(path, encoding=\"utf-8\") as file:\n",
        "        for i, data in enumerate(file):\n",
        "            if not keep_case:\n",
        "                data = str(data).strip().lower()\n",
        "            words = data.strip()\n",
        "            if(len(words)>max_sentence_length):\n",
        "                trimed_sentence_count += 1\n",
        "            words_trimed = words[:max_sentence_length]\n",
        "            if words_trimed:\n",
        "                All += [[Constants.BOS] + words_trimed + [Constants.EOS]]\n",
        "            else:\n",
        "                All += [None]\n",
        "    return All\n",
        "def build_word2index(path1, path2):\n",
        "    word2index = {\n",
        "        Constants.BOS_WORD: Constants.BOS,\n",
        "        Constants.EOS_WORD: Constants.EOS,\n",
        "        Constants.PAD_WORD: Constants.PAD,\n",
        "        Constants.UNK_WORD: Constants.UNK}\n",
        "    with open(path1, encoding=\"utf-8\") as file:\n",
        "        for i, data in enumerate(file):\n",
        "            if (word2index.get(str(data).strip())==None):\n",
        "                word2index[str(data).strip()] = len(word2index)\n",
        "            else:\n",
        "                continue\n",
        "    with open(path2, encoding=\"utf-8\") as file:\n",
        "        for i, data in enumerate(file):\n",
        "            if (word2index.get(str(data).strip())==None):\n",
        "                word2index[str(data).strip()] = len(word2index)\n",
        "            else:\n",
        "                continue\n",
        "    with open(\"/content/drive/My Drive/wmt2016/wmt2016/word2index.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "        w2i = json.dumps(word2index)\n",
        "        file.write(w2i)\n",
        "    return  word2index\n",
        "def convert_word_2_index(sentence, word2index):\n",
        "    return [word2index.get(k, Constants.UNK) for k in sentence]\n",
        "class subDataset(Dataset.Dataset):\n",
        "    def __init__(self, source, target):\n",
        "        super(subDataset, self).__init__()\n",
        "        self.source = open(source, \"r\", encoding=\"utf-8\")\n",
        "        self.target = open(target, \"r\", encoding=\"utf-8\")\n",
        "        self.len = 0\n",
        "        a = 0\n",
        "        for i,data in enumerate(self.source):\n",
        "            a = i\n",
        "        self.len = a+1\n",
        "        self.source = open(source, \"r\", encoding=\"utf-8\")\n",
        "        with open(r\"/content/drive/My Drive/wmt2016/wmt2016/word2index.json\", encoding=\"utf-8\") as file:\n",
        "            self.w2i = [json.loads(k) for k in file][0]\n",
        "        # with open(\"./data/DE_word2index.json\", encoding=\"utf-8\") as file:\n",
        "        #     self.dew2i = [json.loads(k) for k in file][0]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    def __getitem__(self, item):\n",
        "        source = convert_word_2_index(str(self.source.readline()).strip().split(), self.w2i)\n",
        "        source_len = len(source)\n",
        "        target = convert_word_2_index(str(self.target.readline()).strip().split(), self.w2i)\n",
        "        target_len = len(target)\n",
        "        return source, source_len, target, target_len\n",
        "class Dataloader():\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.len = dataset.len\n",
        "    def get_batch(self):\n",
        "        source = []\n",
        "        source_len = []\n",
        "        target_input = []\n",
        "        target_len_input = []\n",
        "        target_output = []\n",
        "        target_len_output = []\n",
        "        for i in range(self.batch_size):\n",
        "            s1, sl1, t1, tl1 = self.dataset.__getitem__(1)\n",
        "            source.append(s1)\n",
        "            source_len.append(sl1)\n",
        "            target_input.append(t1[:-1])\n",
        "            target_len_input.append(tl1-1)\n",
        "            target_output.append(t1[1:])\n",
        "            target_len_output.append(tl1-1)\n",
        "        source_max_len = np.max(source_len)\n",
        "        target_max_len_input = np.max(target_len_input)\n",
        "        target_max_len_output = np.max(target_len_output)\n",
        "        #print(source_max_len, target_max_len_input)\n",
        "        source = [k + list(np.zeros(source_max_len-len(k))) for k in source]\n",
        "        target_input = [k + list(np.zeros(target_max_len_input-len(k))) for k in target_input]\n",
        "        target_output = [k + list(np.zeros(target_max_len_output-len(k))) for k in target_output]\n",
        "        source_len = [source_max_len for _ in range(len(source))]\n",
        "        target_len_input = [target_max_len_input for _ in range(len(target_input))]\n",
        "        target_len_output = [target_max_len_output for _ in range(len(target_output))]\n",
        "        return torch.LongTensor(source).cuda(), torch.LongTensor(source_len).cuda(), torch.LongTensor(target_input).cuda(), torch.LongTensor(target_len_input).cuda(), torch.LongTensor(target_output).cuda(), torch.LongTensor(target_len_output).cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlOi33PdiFuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ARG():\n",
        "  def __init__(self):\n",
        "    self.batch_size = 80\n",
        "    self.model_dim = 512\n",
        "    self.epochs = 10\n",
        "    self.head_count = 8\n",
        "    self.dropout = 0.1\n",
        "    self.save_model = \"/content/drive/My Drive/wmt2016/save_model\"\n",
        "    self.label_smoothing = True\n",
        "    self.learning_rate = 0.1\n",
        "    self.max_sentence_length = 110\n",
        "    self.num_layer = 6\n",
        "    self.ffn_dim = 2048\n",
        "    self.warm_up_steps = 8000\n",
        "    self.smoothing = 0.1\n",
        "    self.source_vocab_size = 50000+1\n",
        "    self.target_vocab_size = 50000+1\n",
        "    self.vocab_size = 37007+1\n",
        "opt = ARG()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUHIjIuKiSzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "\n",
        "    #build_word2index(\"./data/vocab.50K.en\", \"./data/vocab.50K.de\")\n",
        "    with open(r\"/content/drive/My Drive/wmt2016/wmt2016/index2word.json\", encoding=\"utf-8\") as file:\n",
        "        index2word = [json.loads(k) for k in file][0]\n",
        "\n",
        "    model = transformer(opt.vocab_size, opt.max_sentence_length, opt.num_layer, opt.model_dim, opt.head_count, opt.ffn_dim, opt.dropout)\n",
        "\n",
        "    print('# generator parameters:', sum(param.numel() for param in model.parameters()))\n",
        "    device = torch.device(\"cuda\")\n",
        "    # model = torch.nn.DataParallel(model, [0,1])\n",
        "    model.to(device)\n",
        "    model_state_dict = model.state_dict()\n",
        "    optimizer = ScheduledOptim(\n",
        "        optim.Adam(\n",
        "            filter(lambda x: x.requires_grad, model.parameters()),\n",
        "            betas=(0.9, 0.98), eps=1e-09),\n",
        "        opt.model_dim, opt.warm_up_steps)\n",
        "    # checkpoint = {\n",
        "    #     'model': model_state_dict,\n",
        "    #     'settings': opt,\n",
        "    #     'epoch': 0}\n",
        "    # torch.save(checkpoint, \"./Transformer%d\" % 0)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr = 0.9)\n",
        "    for j in range(opt.epochs):\n",
        "\n",
        "        dataset = subDataset(r\"/content/drive/My Drive/wmt2016/wmt2016/train_cut50.en\", r\"/content/drive/My Drive/wmt2016/wmt2016/train_cut50.de\")\n",
        "        dataloader = Dataloader(dataset, opt.batch_size)\n",
        "        bar = tqdm(range(dataset.len // opt.batch_size))\n",
        "        for i in bar:\n",
        "            optimizer.zero_grad()\n",
        "            source, source_len, target_input, target_len_input, target_output, target_len_output = dataloader.get_batch()\n",
        "            output, encoder_attention, decoder_self_attention, decoder_encoder_decoder_attention = model(source, source_len, target_input, target_len_input)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            L = loss(output, target_output, opt.smoothing, opt.target_vocab_size)\n",
        "            file = open(\"/content/drive/My Drive/wmt2016/wmt2016/1.txt\", \"a\", encoding=\"utf-8\")\n",
        "            file.write(str(L.item())+\"\\n\")\n",
        "            if(i%100 == 0):\n",
        "                bleu = get_bleu(output, target_output, index2word)\n",
        "                print(bleu)\n",
        "                file.write(str(L.item())+\" \"+ str(bleu)+\"\\n\")\n",
        "            bar.set_description(\"loss:%f\" % (L.item()))\n",
        "            \n",
        "            \n",
        "            L.backward()\n",
        "            optimizer.step_and_update_lr()\n",
        "            if(i % 10000 == 0):\n",
        "                model_state_dict = model.state_dict()\n",
        "                checkpoint = {\n",
        "                    'model': model_state_dict,\n",
        "                    'settings': opt,\n",
        "                    'epoch': j}\n",
        "                torch.save(checkpoint, \"/content/drive/My Drive/wmt2016/save_model/Transformer%d_%d\" % (j, i))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HawkbWV_rlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9f257b5-8705-423d-ab35-a955a5de2371"
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MJHnZMFiXQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "1a377e92-0619-40cb-a080-3425467a7249"
      },
      "source": [
        "train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# generator parameters: 63200256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-7e02e5821286>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"/content/drive/My Drive/wmt2016/wmt2016/train_cut50.en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"/content/drive/My Drive/wmt2016/wmt2016/train_cut50.de\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-89434dc02248>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, target)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpYxqZB5il_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import json\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoCZD5OBofUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = \"where is shanghai?\"\n",
        "b = \"where is beijing?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94-IpF2IolNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_bleu([a], b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5rHcjerooj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}